{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchain_text_splitters faiss-cpu qdrant-client protonx dotenv langchain-google-genai pyvi rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426bd57",
   "metadata": {},
   "source": [
    "# I. Load documents (Enhanced with metadata preservation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf90ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING MODE: Loading limited documents for faster testing\n",
      "ÔøΩÔøΩ Will load maximum 50 documents\n",
      "============================================================\n",
      "ÔøΩÔøΩ Loading documents (max 50 for testing)...\n",
      "üìÑ Loaded 10/50 documents...\n",
      "üìÑ Loaded 20/50 documents...\n",
      "üìÑ Loaded 30/50 documents...\n",
      "üìÑ Loaded 40/50 documents...\n",
      "üìÑ Loaded 50/50 documents...\n",
      "‚úÖ Reached limit of 50 documents for testing\n",
      "\n",
      "‚úÖ Enhanced loading: Loaded 50 structured documents with metadata\n",
      "üìã Sample metadata: {'S·ªë hi·ªáu': '05/CT-UBND', 'Lo·∫°i vƒÉn b·∫£n': 'Ch·ªâ th·ªã', 'N∆°i ban h√†nh': 'Th√†nh ph·ªë C·∫ßn Th∆°', 'Ng∆∞·ªùi k√Ω': 'V∆∞∆°ng Qu·ªëc Nam', 'Ng√†y ban h√†nh': '26/08/2025', 'Ng√†y hi·ªáu l·ª±c': 'ƒê√£ bi·∫øt', 'Ng√†y c√¥ng b√°o': 'ƒêang c·∫≠p nh·∫≠t', 'S·ªë c√¥ng b√°o': 'ƒêang c·∫≠p nh·∫≠t', 'T√¨nh tr·∫°ng': 'ƒê√£ bi·∫øt', 'url': 'https://thuvienphapluat.vn/van-ban/Bo-may-hanh-chinh/Chi-thi-05-CT-UBND-2025-day-manh-viec-chap-hanh-quy-dinh-phap-luat-ve-gia-Can-Tho-670694.aspx', 'title': 'Ch·ªâ th·ªã 05/CT-UBND 2025 ƒë·∫©y m·∫°nh vi·ªác ch·∫•p h√†nh quy ƒë·ªãnh ph√°p lu·∫≠t v·ªÅ gi√° C·∫ßn Th∆°', 'source': '../data/raw_data/documents\\\\Ch·ªâ th·ªã 05CT-UBND 2025 ƒë·∫©y m·∫°nh vi·ªác ch·∫•p h√†nh quy ƒë·ªãnh ph√°p lu·∫≠t v·ªÅ gi√° C·∫ßn Th∆°\\\\content.txt', 'doc_id': 1}\n",
      "üìä Document range: 1-50\n",
      "\n",
      "üìÑ First document preview:\n",
      "Ti√™u ƒë·ªÅ: Ch·ªâ th·ªã 05/CT-UBND 2025 ƒë·∫©y m·∫°nh vi·ªác ch·∫•p h√†nh quy ƒë·ªãnh ph√°p lu·∫≠t v·ªÅ gi√° C·∫ßn Th∆°\n",
      "\n",
      "T√≥m t·∫Øt: \n",
      "\n",
      "To√†n vƒÉn: ·ª¶Y BAN NH√ÇN D√ÇN TH√ÄNH PH·ªê C·∫¶N TH∆† ------- C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM ƒê·ªôc l·∫≠p - T·ª± do - H·∫°nh ph√∫c --------------- S·ªë: 05/CT-UBND C·∫ßn Th∆°, ng√†y 26 th√°ng 8 nƒÉm 2025\n",
      "\n",
      "·ª¶Y BAN NH√ÇN D√ÇN \n",
      "\n",
      "ÔøΩÔøΩ TESTING SUMMARY:\n",
      "üìä Total documents loaded: 50\n",
      "‚è±Ô∏è  Ready for testing with reduced dataset\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Enhanced document loading with metadata preservation - TESTING VERSION (50 docs only)\n",
    "import os\n",
    "import json\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_legal_docs_from_folders(root_dir: str, max_docs: int = 50) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Enhanced loader that preserves structured metadata from folders containing\n",
    "    both 'metadata.json' and 'content.txt' files.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Directory to load documents from\n",
    "        max_docs: Maximum number of documents to load (for testing)\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    doc_count = 0\n",
    "    \n",
    "    print(f\"ÔøΩÔøΩ Loading documents (max {max_docs} for testing)...\")\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if doc_count >= max_docs:\n",
    "            print(f\"‚úÖ Reached limit of {max_docs} documents for testing\")\n",
    "            break\n",
    "            \n",
    "        if \"metadata.json\" in filenames and \"content.txt\" in filenames:\n",
    "            metadata_path = os.path.join(dirpath, \"metadata.json\")\n",
    "            content_path = os.path.join(dirpath, \"content.txt\")\n",
    "\n",
    "            try:\n",
    "                with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "                    metadata_json = json.load(f)\n",
    "                with open(content_path, 'r', encoding='utf-8') as f:\n",
    "                    full_text = f.read()\n",
    "\n",
    "                # Enhanced page content with title and summary\n",
    "                page_content = (\n",
    "                    f\"Ti√™u ƒë·ªÅ: {metadata_json.get('title', '')}\\n\\n\"\n",
    "                    f\"T√≥m t·∫Øt: {metadata_json['metadata'].get('t√≥m t·∫Øt vƒÉn b·∫£n', '')}\\n\\n\"\n",
    "                    f\"To√†n vƒÉn: {full_text}\"\n",
    "                )\n",
    "\n",
    "                # Preserve important metadata\n",
    "                final_metadata = metadata_json.get(\"metadata\", {}).get(\"thu·ªôc t√≠nh\", {})\n",
    "                final_metadata['url'] = metadata_json.get('url', '')\n",
    "                final_metadata['title'] = metadata_json.get('title', '')\n",
    "                final_metadata['source'] = content_path\n",
    "                final_metadata['doc_id'] = doc_count + 1  # Add document ID for testing\n",
    "                \n",
    "                doc = Document(page_content=page_content, metadata=final_metadata)\n",
    "                documents.append(doc)\n",
    "                doc_count += 1\n",
    "                \n",
    "                # Show progress every 10 documents\n",
    "                if doc_count % 10 == 0:\n",
    "                    print(f\"üìÑ Loaded {doc_count}/{max_docs} documents...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing files in {dirpath}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# TESTING CONFIGURATION - Load only a few documents\n",
    "root_dir = '../data/raw_data/documents'\n",
    "MAX_DOCS_FOR_TESTING = 50 \n",
    "\n",
    "print(\"üß™ TESTING MODE: Loading limited documents for faster testing\")\n",
    "print(f\"ÔøΩÔøΩ Will load maximum {MAX_DOCS_FOR_TESTING} documents\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    docs = load_legal_docs_from_folders(root_dir, max_docs=MAX_DOCS_FOR_TESTING)\n",
    "    print(f'\\n‚úÖ Enhanced loading: Loaded {len(docs)} structured documents with metadata')\n",
    "    if docs:\n",
    "        print(\"üìã Sample metadata:\", docs[0].metadata)\n",
    "        print(f\"üìä Document range: 1-{len(docs)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Enhanced loading failed: {e}\")\n",
    "    print(\"üîÑ Falling back to simple loading...\")\n",
    "    \n",
    "    # Fallback to simple loading with limit\n",
    "    loader = DirectoryLoader(\n",
    "        root_dir, \n",
    "        glob='**/*.txt', \n",
    "        loader_cls=TextLoader, \n",
    "        show_progress=True\n",
    "    )\n",
    "    all_docs = loader.load()\n",
    "    docs = all_docs[:MAX_DOCS_FOR_TESTING]  # Take only first 100\n",
    "    print(f'‚úÖ Simple loading: Loaded {len(docs)} documents (limited to {MAX_DOCS_FOR_TESTING})')\n",
    "\n",
    "print(f'\\nüìÑ First document preview:')\n",
    "print(docs[0].page_content[:300])\n",
    "\n",
    "print(f\"\\nÔøΩÔøΩ TESTING SUMMARY:\")\n",
    "print(f\"üìä Total documents loaded: {len(docs)}\")\n",
    "print(f\"‚è±Ô∏è  Ready for testing with reduced dataset\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664275df",
   "metadata": {},
   "source": [
    "# II. Enhanced Document Splitting (Parent-Child Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d42a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02d8817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up parent-child document architecture...\n",
      "Created 2261 child chunks from 50 documents\n",
      "Parent chunks stored in docstore for context retrieval\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Parent-Child Architecture for better context preservation\n",
    "print(\"Setting up parent-child document architecture...\")\n",
    "\n",
    "# Parent splitter: larger chunks for context\n",
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, \n",
    "    chunk_overlap=300,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Child splitter: smaller chunks for retrieval\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400, \n",
    "    chunk_overlap=100,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Create parent chunks and store them\n",
    "docstore = InMemoryStore()\n",
    "child_chunks = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Split into parent chunks\n",
    "    parent_chunks = parent_splitter.split_documents([doc])\n",
    "    \n",
    "    for i, parent_chunk in enumerate(parent_chunks):\n",
    "        parent_id = f\"{doc.metadata.get('title', 'doc')}-{i}\"\n",
    "        docstore.mset([(parent_id, parent_chunk)])\n",
    "        \n",
    "        # Split parent into child chunks\n",
    "        sub_chunks = child_splitter.split_documents([parent_chunk])\n",
    "        for sub_chunk in sub_chunks:\n",
    "            sub_chunk.metadata['parent_id'] = parent_id\n",
    "            # Preserve original metadata\n",
    "            sub_chunk.metadata.update(doc.metadata)\n",
    "            child_chunks.append(sub_chunk)\n",
    "\n",
    "print(f\"Created {len(child_chunks)} child chunks from {len(docs)} documents\")\n",
    "print(f\"Parent chunks stored in docstore for context retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80614a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent-child architecture setup complete!\n",
      "Child chunks for retrieval: 2261\n",
      "Average child chunk size: 608 characters\n"
     ]
    }
   ],
   "source": [
    "print(f'Parent-child architecture setup complete!')\n",
    "print(f'Child chunks for retrieval: {len(child_chunks)}')\n",
    "print(f'Average child chunk size: {sum(len(chunk.page_content) for chunk in child_chunks) // len(child_chunks)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb32ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child chunk sizes (first 10): [547, 664, 747, 635, 393, 485, 455, 489, 656, 623]\n",
      "Min: 25, Max: 883, Avg: 608\n"
     ]
    }
   ],
   "source": [
    "# Show chunk size distribution\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in child_chunks]\n",
    "print(f\"Child chunk sizes (first 10): {chunk_sizes[:10]}\")\n",
    "print(f\"Min: {min(chunk_sizes)}, Max: {max(chunk_sizes)}, Avg: {sum(chunk_sizes)//len(chunk_sizes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60036ecb",
   "metadata": {},
   "source": [
    "# III. Enhanced Embedding with ProtonX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d77af67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from protonx import ProtonX\n",
    "\n",
    "load_dotenv()  # take environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fbef235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtonXEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    A custom LangChain embeddings class for the ProtonX API.\n",
    "    This wrapper handles authentication and batching to prevent token limit errors.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size: int = 8, max_retries: int = 3, delay: int = 5):\n",
    "        \"\"\"Initializes the ProtonX client and sets the batch size.\"\"\"\n",
    "        self.client = ProtonX()  # Automatically uses PROTONX_API_KEY from environment\n",
    "        self.batch_size = batch_size\n",
    "        self.max_retries = max_retries\n",
    "        self.delay = delay\n",
    "        print(f\"ProtonXEmbeddings initialized with batch size: {self.batch_size}\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generates embeddings for a list of documents, processing them in batches\n",
    "        to stay within the API's token limits.\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    response = self.client.embeddings.create(input=batch)\n",
    "                    if not response or not response.get(\"data\"):\n",
    "                        raise ValueError(f\"Failed to get embeddings from ProtonX API for batch starting at index {i}\")\n",
    "\n",
    "                    batch_embeddings = [item.get(\"embedding\") for item in response.get(\"data\")]\n",
    "                    all_embeddings.extend(batch_embeddings)\n",
    "                    break\n",
    "\n",
    "                except ConnectionError as e:\n",
    "                    print(f\"Connection error on attempt {attempt + 1}/{self.max_retries}. Retrying in {self.delay} seconds...\")\n",
    "                    if attempt + 1 == self.max_retries:\n",
    "                        raise e\n",
    "                    time.sleep(self.delay)\n",
    "        return all_embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Generates an embedding for a single query text.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.client.embeddings.create(text)\n",
    "                if not response or not response.get(\"data\"):\n",
    "                    raise ValueError(\"Failed to get embedding from ProtonX API for the query\")\n",
    "\n",
    "                return response.get(\"data\")[0].get(\"embedding\")\n",
    "            except ConnectionError as e:\n",
    "                print(f\"Connection eror on attempt {attempt + 1}/{self.max_retries}. Retrying in {self.delay} seconds...\")\n",
    "                if attempt + 1 == self.max_retries:\n",
    "                    raise e\n",
    "                time.sleep(self.delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebae4c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtonXEmbeddings initialized with batch size: 15\n",
      "ProtonX embeddings object created and ready to use.\n"
     ]
    }
   ],
   "source": [
    "embeddings = ProtonXEmbeddings(batch_size=15)\n",
    "print(\"ProtonX embeddings object created and ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Set your ProtonX API key\n",
    "# os.environ[\"PROTONX_API_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6Im5wcWFjZWR1QGdtYWlsLmNvbSIsImlhdCI6MTc1NzkwNTY1NiwiZXhwIjoxNzYwNDk3NjU2fQ.7vSGzeMSRAbdzrWSZikUt5Xftr1BMQb_OEoi577R39s\"\n",
    "\n",
    "# # Then initialize the client\n",
    "# from protonx import ProtonX\n",
    "# client = ProtonX()\n",
    "\n",
    "# # Test the connection\n",
    "# result = client.embeddings.create(\"T√¥i y√™u Vi·ªát Nam ƒë·∫Øm say hihi!!!\")\n",
    "# print(\"Success:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb08a5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Starting embedding process for 2261 child chunks...\n",
      "üîß Using batch size: 15\n",
      "‚è±Ô∏è  Estimated time: 301.5 seconds (rough estimate)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2261/2261 [34:54<00:00,  1.08chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ EMBEDDING PROCESS COMPLETED!\n",
      "============================================================\n",
      "ÔøΩÔøΩ Total chunks processed: 2261\n",
      "‚è±Ô∏è  Total time: 2094.16 seconds (34.9 minutes)\n",
      "‚ö° Average speed: 0.93 seconds per chunk\n",
      "ÔøΩÔøΩ Throughput: 1.1 chunks per second\n",
      "ÔøΩÔøΩ Embedding dimension: 768\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Extract texts and metadatas from child chunks\n",
    "texts = [chunk.page_content for chunk in child_chunks]\n",
    "metadatas = [chunk.metadata for chunk in child_chunks]\n",
    "\n",
    "print(f\"üìä Starting embedding process for {len(texts)} child chunks...\")\n",
    "print(f\"üîß Using batch size: {embeddings.batch_size}\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: {len(texts) / embeddings.batch_size * 2:.1f} seconds (rough estimate)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 2. Simple progress bar like the image\n",
    "start_time = time.time()\n",
    "\n",
    "# Process embeddings with simple progress bar\n",
    "text_embeddings = []\n",
    "total_chunks = len(texts)\n",
    "\n",
    "# Create a simple progress bar that matches the image style\n",
    "with tqdm(total=total_chunks, desc=\"Embedding chunks\", unit=\"chunk\") as pbar:\n",
    "    \n",
    "    for i in range(0, len(texts), embeddings.batch_size):\n",
    "        batch = texts[i:i + embeddings.batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Process batch\n",
    "            batch_embeddings = embeddings.embed_documents(batch)\n",
    "            text_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(len(batch))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error in batch {i//embeddings.batch_size + 1}: {e}\")\n",
    "            raise e\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ EMBEDDING PROCESS COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ÔøΩÔøΩ Total chunks processed: {len(text_embeddings)}\")\n",
    "print(f\"‚è±Ô∏è  Total time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\")\n",
    "print(f\"‚ö° Average speed: {total_time/len(text_embeddings):.2f} seconds per chunk\")\n",
    "print(f\"ÔøΩÔøΩ Throughput: {len(text_embeddings)/total_time:.1f} chunks per second\")\n",
    "print(f\"ÔøΩÔøΩ Embedding dimension: {len(text_embeddings[0]) if text_embeddings else 'N/A'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "731e3568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding dimension is: 768\n"
     ]
    }
   ],
   "source": [
    "# Embed a sample text to find the dimension\n",
    "sample_embedding = embeddings.embed_query(\"this is a test\")\n",
    "dimension = len(sample_embedding)\n",
    "print(f\"The embedding dimension is: {dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ca5f2",
   "metadata": {},
   "source": [
    "# IV. Enhanced Vector Stores (FAISS + BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c91622f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating FAISS vector store...\n",
      "FAISS vector store created successfully.\n",
      "\n",
      "Setting up BM25 retriever with Vietnamese tokenization...\n",
      "BM25 retriever with Vietnamese tokenization created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Setup FAISS vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"\\nCreating FAISS vector store...\")\n",
    "# FAISS requires (text, embedding) pairs\n",
    "text_embedding_pairs = list(zip(texts, text_embeddings))\n",
    "vectorstore_local = FAISS.from_embeddings(text_embedding_pairs, embeddings, metadatas=metadatas)\n",
    "print(\"FAISS vector store created successfully.\")\n",
    "\n",
    "# Setup BM25 retriever with Vietnamese tokenization\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "print(\"\\nSetting up BM25 retriever with Vietnamese tokenization...\")\n",
    "\n",
    "def tokenize_vi_for_bm25(doc):\n",
    "    \"\"\"Tokenize Vietnamese text for better BM25 matching\"\"\"\n",
    "    doc.page_content = ViTokenizer.tokenize(doc.page_content)\n",
    "    return doc\n",
    "\n",
    "# Create tokenized copies for BM25\n",
    "child_chunks_tokenized = [tokenize_vi_for_bm25(chunk) for chunk in child_chunks]\n",
    "bm25_retriever = BM25Retriever.from_documents(child_chunks_tokenized)\n",
    "bm25_retriever.k = 15  # Retrieve top 15 for ensemble\n",
    "print(\"BM25 retriever with Vietnamese tokenization created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b1c0350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up ensemble retrieval...\n",
      "Ensemble retriever (BM25 + FAISS) created successfully.\n",
      "Parent document retrieval chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Setup Ensemble Retrieval (BM25 + FAISS)\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "print(\"\\nSetting up ensemble retrieval...\")\n",
    "\n",
    "# Create FAISS retriever\n",
    "faiss_retriever = vectorstore_local.as_retriever(search_kwargs={'k': 15})\n",
    "\n",
    "# Create ensemble retriever combining BM25 and FAISS\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever],\n",
    "    weights=[0.6, 0.4]  # Favor BM25 for Vietnamese lexical matching\n",
    ")\n",
    "\n",
    "print(\"Ensemble retriever (BM25 + FAISS) created successfully.\")\n",
    "\n",
    "# Setup Parent Document Retrieval Chain\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def _get_parent_docs(input_dict: dict) -> list[Document]:\n",
    "    \"\"\"Retrieve parent documents from child documents\"\"\"\n",
    "    child_docs = input_dict[\"child_docs\"]\n",
    "    store = input_dict[\"docstore\"]\n",
    "    parent_ids = []\n",
    "    for doc in child_docs:\n",
    "        if \"parent_id\" in doc.metadata and doc.metadata[\"parent_id\"] not in parent_ids:\n",
    "            parent_ids.append(doc.metadata[\"parent_id\"])\n",
    "    return [doc for doc in store.mget(parent_ids) if doc is not None]\n",
    "\n",
    "# Create the parent retrieval chain\n",
    "parent_retriever_chain = (\n",
    "    {\"child_docs\": ensemble_retriever}\n",
    "    | RunnablePassthrough.assign(docstore=lambda x: docstore)\n",
    "    | RunnableLambda(_get_parent_docs)\n",
    ")\n",
    "\n",
    "print(\"Parent document retrieval chain created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c28bc",
   "metadata": {},
   "source": [
    "# V. Enhanced Retrieval Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f9a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hybrid retrieval for query: 'nh√† n∆∞·ªõc t·∫∑ng bao nhi√™u ti·ªÅn cho m·ªôt ng∆∞·ªùi?'\n",
      "\n",
      "=== Ensemble Retrieval (Child Chunks) ===\n",
      "Retrieved 29 child chunks\n",
      "\n",
      "=== Parent Document Retrieval ===\n",
      "Retrieved 16 parent documents\n",
      "\n",
      "--- Sample Parent Document ---\n",
      "Title: C√¥ng ƒëi·ªán 149/Cƒê-TTg 2025 t·∫∑ng qu√† ng∆∞·ªùi d√¢n d·ªãp k·ª∑ ni·ªám 80 nƒÉm C√°ch m·∫°ng Th√°ng T√°m Qu·ªëc kh√°nh 2/9\n",
      "Content preview: To√†n vƒÉn: TH·ª¶ T∆Ø·ªöNG CH√çNH PH·ª¶ -------- C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM ƒê·ªôc l·∫≠p - T·ª± do - H·∫°nh ph√∫c --------------- S·ªë: 149/Cƒê-TTg H√† N·ªôi, ng√†y 28 th√°ng 8 nƒÉm 2025\n",
      "\n",
      "TH·ª¶ T∆Ø·ªöNG CH√çNH PH·ª¶ --------\n",
      "\n",
      "C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM ƒê·ªôc l·∫≠p - T·ª± do - H·∫°nh ph√∫c ---------------\n",
      "\n",
      "S·ªë: 149/Cƒê-TTg\n",
      "\n",
      "H√† N·ªôi,...\n",
      "Source: ../data/raw_data/documents\\C√¥ng ƒëi·ªán 149Cƒê-TTg 2025 t·∫∑ng qu√† ng∆∞·ªùi d√¢n d·ªãp k·ª∑ ni·ªám 80 nƒÉm C√°ch m·∫°ng Th√°ng T√°m Qu·ªëc kh√°nh 29\\content.txt\n"
     ]
    }
   ],
   "source": [
    "# Test the enhanced hybrid retrieval\n",
    "query = \"nh√† n∆∞·ªõc t·∫∑ng bao nhi√™u ti·ªÅn cho m·ªôt ng∆∞·ªùi?\"\n",
    "\n",
    "print(f\"Testing hybrid retrieval for query: '{query}'\\n\")\n",
    "\n",
    "# Test ensemble retrieval (child chunks)\n",
    "print(\"=== Ensemble Retrieval (Child Chunks) ===\")\n",
    "ensemble_docs = ensemble_retriever.invoke(query)\n",
    "print(f\"Retrieved {len(ensemble_docs)} child chunks\")\n",
    "\n",
    "# Test parent document retrieval\n",
    "print(\"\\n=== Parent Document Retrieval ===\")\n",
    "parent_docs = parent_retriever_chain.invoke(query)\n",
    "print(f\"Retrieved {len(parent_docs)} parent documents\")\n",
    "\n",
    "# Show sample results\n",
    "print(f\"\\n--- Sample Parent Document ---\")\n",
    "if parent_docs:\n",
    "    print(f\"Title: {parent_docs[0].metadata.get('title', 'N/A')}\")\n",
    "    print(f\"Content preview: {parent_docs[0].page_content[:300]}...\")\n",
    "    print(f\"Source: {parent_docs[0].metadata.get('source', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c44a6",
   "metadata": {},
   "source": [
    "# VI. Enhanced RAG with Hybrid Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0fc1e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\installedApps\\miniconda3\\envs\\fyp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f95a24c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced RAG chain with hybrid retrieval and parent documents created successfully.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0)\n",
    "\n",
    "# Enhanced prompt template with Vietnamese instructions and citation requirements\n",
    "template = \"\"\"Answer the question based ONLY on the following context.\n",
    "Your answer must be in Vietnamese.\n",
    "Your answer should be well-structured and easy to read.\n",
    "- Use bullet points or numbered lists for multiple items or steps.\n",
    "- Use **bold** for key terms, names, or important numbers and concepts.\n",
    "- Use *italics* for emphasis or to highlight specific terms.\n",
    "\n",
    "**After providing the answer, you MUST cite your sources accurately using the metadata from the context.**\n",
    "For each source used, provide its title and document number (S·ªë hi·ªáu) if available.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the context does not provide enough information, say \"T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.\" and do not provide an answer.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents with metadata for better context\"\"\"\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        # Add document metadata as header\n",
    "        title = doc.metadata.get('title', f'Document {i+1}')\n",
    "        source = doc.metadata.get('source', 'Unknown source')\n",
    "        formatted_doc = f\"--- Document: {title} ---\\n{doc.page_content}\\n--- End Document ---\"\n",
    "        formatted_docs.append(formatted_doc)\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "# Enhanced RAG chain using parent document retrieval\n",
    "rag_chain = (\n",
    "    {\"context\": parent_retriever_chain | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Enhanced RAG chain with hybrid retrieval and parent documents created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70bf36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: nh√† n∆∞·ªõc t·∫∑ng bao nhi√™u ti·ªÅn cho m·ªôt ng∆∞·ªùi?\n",
      "\n",
      "================================================================================\n",
      "Response: D·ª±a tr√™n th√¥ng tin t·ª´ c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p, nh√¢n d·ªãp k·ª∑ ni·ªám **80 nƒÉm C√°ch m·∫°ng Th√°ng T√°m v√† Qu·ªëc kh√°nh 2/9**, nh√† n∆∞·ªõc s·∫Ω t·∫∑ng qu√† cho ng∆∞·ªùi d√¢n v·ªõi m·ª©c c·ª• th·ªÉ nh∆∞ sau:\n",
      "\n",
      "*   **M·ª©c qu√† t·∫∑ng**: **100.000 ƒë·ªìng/ng∆∞·ªùi d√¢n**.\n",
      "\n",
      "M√≥n qu√† n√†y ƒë∆∞·ª£c trao *cho to√†n d√¢n ƒÉn T·∫øt ƒê·ªôc l·∫≠p* v√† ph·∫£i ƒë∆∞·ª£c chuy·ªÉn ƒë·∫øn ng∆∞·ªùi d√¢n xong tr∆∞·ªõc ng√†y Qu·ªëc kh√°nh 02/9/2025.\n",
      "\n",
      "***\n",
      "\n",
      "**Ngu·ªìn:**\n",
      "1.  C√¥ng ƒëi·ªán 149/Cƒê-TTg 2025 t·∫∑ng qu√† ng∆∞·ªùi d√¢n d·ªãp k·ª∑ ni·ªám 80 nƒÉm C√°ch m·∫°ng Th√°ng T√°m Qu·ªëc kh√°nh 2/9 (S·ªë hi·ªáu: 149/Cƒê-TTg)\n",
      "2.  C√¥ng ƒëi·ªán 09/Cƒê-KBNN 2025 qu√°n tri·ªát ch·ªâ ƒë·∫°o th·ª±c hi·ªán K·∫øt lu·∫≠n 183/KT-TW v√† 260/NQ-CP (S·ªë hi·ªáu: 09/Cƒê-KBNN)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the enhanced RAG system\n",
    "query = \"nh√† n∆∞·ªõc t·∫∑ng bao nhi√™u ti·ªÅn cho m·ªôt ng∆∞·ªùi?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c110631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: nh√† n∆∞·ªõc t·∫∑ng qu√† cho d√¢n nh√¢n d·ªãp g√¨?\n",
      "\n",
      "================================================================================\n",
      "Response: D·ª±a tr√™n th√¥ng tin t·ª´ c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p, Nh√† n∆∞·ªõc t·∫∑ng qu√† cho ng∆∞·ªùi d√¢n nh√¢n d·ªãp:\n",
      "\n",
      "*   **K·ª∑ ni·ªám 80 nƒÉm C√°ch m·∫°ng Th√°ng T√°m v√† Qu·ªëc kh√°nh 2/9**.\n",
      "\n",
      "---\n",
      "**Ngu·ªìn:**\n",
      "*   C√¥ng ƒëi·ªán 149/Cƒê-TTg 2025 t·∫∑ng qu√† ng∆∞·ªùi d√¢n d·ªãp k·ª∑ ni·ªám 80 nƒÉm C√°ch m·∫°ng Th√°ng T√°m Qu·ªëc kh√°nh 2/9, S·ªë hi·ªáu: 149/Cƒê-TTg.\n",
      "*   C√¥ng ƒëi·ªán 152/C√ê-TTg 2025 kh·∫©n tr∆∞∆°ng tri·ªÉn khai t·∫∑ng qu√† nh√¢n d·ªãp k·ª∑ ni·ªám 80 nƒÉm Qu·ªëc kh√°nh, S·ªë hi·ªáu: 152/Cƒê-TTg.\n",
      "*   C√¥ng ƒëi·ªán 154/Cƒê-TTg 2025 ƒë·∫©y nhanh t·∫∑ng qu√† Nh√¢n d√¢n nh√¢n d·ªãp k·ª∑ ni·ªám Qu·ªëc kh√°nh, S·ªë hi·ªáu: 154/Cƒê-TTg.\n",
      "*   C√¥ng ƒëi·ªán 09/Cƒê-KBNN 2025 qu√°n tri·ªát ch·ªâ ƒë·∫°o th·ª±c hi·ªán K·∫øt lu·∫≠n 183/KT-TW v√† 260/NQ-CP, S·ªë hi·ªáu: 09/Cƒê-KBNN.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with another query\n",
    "query = \"nh√† n∆∞·ªõc t·∫∑ng qu√† cho d√¢n nh√¢n d·ªãp g√¨?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a610be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c630c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: D·ª±a tr√™n h·ªì s∆° do C√¥ng ty TNHH OTES CORPORATION cung c·∫•p, h√£y m√¥ t·∫£ c√°c ƒë·∫∑c t√≠nh k·ªπ thu·∫≠t ch√≠nh c·ªßa s·∫£n ph·∫©m TamSoil Polynite ECO ·ªü tr·∫°ng th√°i dung d·ªãch sau khi pha, v√† n√™u r√µ cƒÉn c·ª© ph√°p l√Ω cao nh·∫•t (Lu·∫≠t) m√† C·ª•c H·∫£i quan ƒë√£ d·ª±a v√†o ƒë·ªÉ ban h√†nh th√¥ng b√°o m√£ s·ªë cho s·∫£n ph·∫©m n√†y.\n",
      "\n",
      "================================================================================\n",
      "Response: T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test with complex legal query\n",
    "query = \"D·ª±a tr√™n h·ªì s∆° do C√¥ng ty TNHH OTES CORPORATION cung c·∫•p, h√£y m√¥ t·∫£ c√°c ƒë·∫∑c t√≠nh k·ªπ thu·∫≠t ch√≠nh c·ªßa s·∫£n ph·∫©m TamSoil Polynite ECO ·ªü tr·∫°ng th√°i dung d·ªãch sau khi pha, v√† n√™u r√µ cƒÉn c·ª© ph√°p l√Ω cao nh·∫•t (Lu·∫≠t) m√† C·ª•c H·∫£i quan ƒë√£ d·ª±a v√†o ƒë·ªÉ ban h√†nh th√¥ng b√°o m√£ s·ªë cho s·∫£n ph·∫©m n√†y.\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Display with markdown formatting\n",
    "display(Markdown(f\"**Response:** {response}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a9efb",
   "metadata": {},
   "source": [
    "# VII. Performance Comparison and Analysis\n",
    "\n",
    "## Enhanced Features Summary\n",
    "\n",
    "### ‚úÖ **What's New in This Enhanced Version:**\n",
    "\n",
    "1. **üîç Hybrid Retrieval**: BM25 (lexical) + FAISS (semantic) ensemble\n",
    "2. **üáªüá≥ Vietnamese Tokenization**: Better BM25 matching for Vietnamese text\n",
    "3. **üìö Parent-Child Architecture**: Better context preservation\n",
    "4. **üìÑ Enhanced Data Loading**: Preserves structured metadata\n",
    "5. **üéØ Improved Prompting**: Vietnamese instructions with citation requirements\n",
    "6. **‚ö° ProtonX Embeddings**: High-quality Vietnamese embeddings (free)\n",
    "\n",
    "### **Architecture Flow:**\n",
    "```\n",
    "Query ‚Üí [BM25 + FAISS] ‚Üí Ensemble ‚Üí Parent-Child Hydration ‚Üí Enhanced RAG ‚Üí Response\n",
    "```\n",
    "\n",
    "### **Key Improvements Over Original:**\n",
    "- **Better Recall**: Hybrid retrieval catches both exact matches and semantic similarity\n",
    "- **Better Context**: Parent-child architecture provides more complete context\n",
    "- **Vietnamese-Optimized**: Tokenization and specialized embeddings\n",
    "- **Production-Ready**: Robust error handling and metadata preservation\n",
    "- **Citation Support**: Structured metadata for accurate source attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c8d595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing simple re-ranking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original retrieval: 16 documents\n",
      "After re-ranking: 3 documents\n",
      "Re-ranking function ready for use if needed.\n"
     ]
    }
   ],
   "source": [
    "# Optional: Add simple re-ranking for even better precision\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def simple_rerank(docs, query, top_k=5):\n",
    "    \"\"\"Simple re-ranking based on query-document similarity\"\"\"\n",
    "    if not docs:\n",
    "        return docs\n",
    "    \n",
    "    # Use the same ProtonX embeddings for consistency\n",
    "    model = SentenceTransformer('dangvantuan/vietnamese-embedding')\n",
    "    \n",
    "    # Get embeddings\n",
    "    query_embedding = model.encode([query])\n",
    "    doc_embeddings = model.encode([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "    \n",
    "    # Sort by similarity and return top-k\n",
    "    doc_sim_pairs = list(zip(docs, similarities))\n",
    "    doc_sim_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [doc for doc, _ in doc_sim_pairs[:top_k]]\n",
    "\n",
    "# Test re-ranking\n",
    "print(\"Testing simple re-ranking...\")\n",
    "test_query = \"nh√† n∆∞·ªõc t·∫∑ng bao nhi√™u ti·ªÅn cho m·ªôt ng∆∞·ªùi?\"\n",
    "retrieved_docs = parent_retriever_chain.invoke(test_query)\n",
    "reranked_docs = simple_rerank(retrieved_docs, test_query, top_k=3)\n",
    "\n",
    "print(f\"Original retrieval: {len(retrieved_docs)} documents\")\n",
    "print(f\"After re-ranking: {len(reranked_docs)} documents\")\n",
    "print(\"Re-ranking function ready for use if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a15045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

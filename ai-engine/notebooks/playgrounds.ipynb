{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc12277d",
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install langchain_community tiktoken langchain_text_splitters faiss-cpu qdrant-client protonx dotenv langchain-google-genai pyvi rank_bm25 numpy sentence-transformers ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0c931c",
      "metadata": {},
      "source": [
        "# I. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9c94153b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "import tiktoken\n",
        "from dotenv import load_dotenv\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from protonx import ProtonX\n",
        "from pyvi import ViTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46add4c8",
      "metadata": {},
      "source": [
        "# II. Load Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "abf90ac1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "   STARTING DOCUMENT LOADING SCRIPT (TESTING MODE)\n",
            "[INFO] Source Directory: ../data/raw_data_50/documents\n",
            "[INFO] Document Limit: 50\n",
            "======================================================================\n",
            "\n",
            "[INFO] Starting enhanced loading from '../data/raw_data_50/documents'...\n",
            "  -> Progress: 50/50 documents loaded.\n",
            "[SUCCESS] Enhanced loading complete. Found 50 structured documents.\n",
            "\n",
            "[DEBUG] Sample metadata from the first document:\n",
            "{\n",
            "  \"_id\": \"66b9c0513ab9c4ae3d5eec2c\",\n",
            "  \"so_hieu\": \"02/2024/NQ-HĐND\",\n",
            "  \"loai_van_ban\": \"Nghị quyết\",\n",
            "  \"linh_vuc_nganh\": \"Giáo dục\",\n",
            "  \"noi_ban_hanh\": \"Thành phố Hải Phòng\",\n",
            "  \"nguoi_ky\": \"Phạm Văn Lập\",\n",
            "  \"ngay_ban_hanh\": \"19/07/2024\",\n",
            "  \"ngay_hieu_luc\": \"01/08/2024\",\n",
            "  \"tinh_trang\": \"Còn hiệu lực\",\n",
            "  \"title\": \"Nghị quyết 02/2024/NQ-HĐND quy định mức thu học phí đối với các cơ sở giáo dục mầm non, giáo dục phổ thông công lập trên địa bàn thành phố Hải Phòng từ năm học 2024-2025\",\n",
            "  \"source\": \"../data/raw_data_50/documents\\\\Nghị quyết 022024NQ-HĐND quy định mức thu học phí đối với các cơ sở giáo dục mầm non, giáo dục phổ t\\\\content.txt\",\n",
            "  \"doc_id\": 1\n",
            "}\n",
            "\n",
            "======================================================================\n",
            "   FINAL SUMMARY\n",
            "  -> Total documents loaded: 50\n",
            "  -> Status: Ready for next processing steps.\n",
            "\n",
            "Preview of first document content:\n",
            "----------------------------------------\n",
            "Tiêu đề: Nghị quyết 02/2024/NQ-HĐND quy định mức thu học phí đối với các cơ sở giáo dục mầm non, giáo dục phổ thông công lập trên địa bàn thành phố Hải Phòng từ năm học 2024-2025\n",
            "\n",
            "Toàn văn: HỘI ĐỒNG NHÂN  DÂN  THÀNH PHỐ HẢI PHÒNG  ------- CỘNG HÒA XÃ HỘI  CHỦ NGHĨA VIỆT NAM  Độc lập - Tự do - Hạnh p...\n",
            "----------------------------------------\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Enhanced document loading with metadata preservation - TESTING VERSION (50 docs only)\n",
        "\n",
        "def load_legal_docs_from_folders(root_dir: str, max_docs: int = 50) -> list[Document]:\n",
        "    \"\"\"\n",
        "    Enhanced loader that preserves structured metadata from folders containing\n",
        "    both 'metadata.json' and 'content.txt' files.\n",
        "    \n",
        "    Args:\n",
        "        root_dir: Directory to load documents from\n",
        "        max_docs: Maximum number of documents to load (for testing)\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    doc_count = 0\n",
        "    \n",
        "    print(f\"\\n[INFO] Starting enhanced loading from '{root_dir}'...\")\n",
        "    \n",
        "    for dirpath, _, filenames in os.walk(root_dir):\n",
        "        if doc_count >= max_docs:\n",
        "            break\n",
        "            \n",
        "        if \"metadata.json\" in filenames and \"content.txt\" in filenames:\n",
        "            metadata_path = os.path.join(dirpath, \"metadata.json\")\n",
        "            content_path = os.path.join(dirpath, \"content.txt\")\n",
        "\n",
        "            try:\n",
        "                with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "                    metadata_json = json.load(f)\n",
        "                with open(content_path, 'r', encoding='utf-8') as f:\n",
        "                    full_text = f.read()\n",
        "\n",
        "                # Enhanced page content with title\n",
        "                page_content = (\n",
        "                    f\"Tiêu đề: {metadata_json.get('title', '')}\\n\\n\"\n",
        "                    f\"Toàn văn: {full_text}\"\n",
        "                )\n",
        "\n",
        "                # Extract specific metadata fields from the diagram section\n",
        "                id_metadata = metadata_json.get(\"metadata\", {})\n",
        "                diagram_metadata = metadata_json.get(\"metadata\", {}).get(\"diagram\", {})\n",
        "                \n",
        "                # Create final metadata with only the required fields\n",
        "                final_metadata = {\n",
        "                    '_id': id_metadata.get('_id', ''),\n",
        "                    'so_hieu': diagram_metadata.get('so_hieu', ''),\n",
        "                    'loai_van_ban': diagram_metadata.get('loai_van_ban', ''),\n",
        "                    'linh_vuc_nganh': diagram_metadata.get('linh_vuc_nganh', ''),\n",
        "                    'noi_ban_hanh': diagram_metadata.get('noi_ban_hanh', ''),\n",
        "                    'nguoi_ky': diagram_metadata.get('nguoi_ky', ''),\n",
        "                    'ngay_ban_hanh': diagram_metadata.get('ngay_ban_hanh', ''),\n",
        "                    'ngay_hieu_luc': diagram_metadata.get('ngay_hieu_luc', ''),\n",
        "                    'tinh_trang': diagram_metadata.get('tinh_trang', ''),\n",
        "                    'title': metadata_json.get('title', ''),\n",
        "                    'source': content_path,\n",
        "                    'doc_id': doc_count + 1  # Add document ID for testing\n",
        "                }\n",
        "                \n",
        "                doc = Document(page_content=page_content, metadata=final_metadata)\n",
        "                documents.append(doc)\n",
        "                doc_count += 1\n",
        "                \n",
        "                # Dynamic progress indicator\n",
        "                progress_message = f\"  -> Progress: {doc_count}/{max_docs} documents loaded.\"\n",
        "                sys.stdout.write('\\r' + progress_message)\n",
        "                sys.stdout.flush()\n",
        "                \n",
        "            except Exception as e:\n",
        "                # Print error on a new line to avoid being overwritten by progress\n",
        "                print(f\"\\n[ERROR] Failed to process document in {dirpath}: {e}\")\n",
        "    \n",
        "    # Print a newline to move past the progress indicator line\n",
        "    print()\n",
        "    return documents\n",
        "\n",
        "# --- SCRIPT EXECUTION ---\n",
        "\n",
        "# TESTING CONFIGURATION - Load only a few documents\n",
        "root_dir = '../data/raw_data_50/documents'\n",
        "MAX_DOCS_FOR_TESTING = 50\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"   STARTING DOCUMENT LOADING SCRIPT (TESTING MODE)\")\n",
        "print(f\"[INFO] Source Directory: {root_dir}\")\n",
        "print(f\"[INFO] Document Limit: {MAX_DOCS_FOR_TESTING}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    docs = load_legal_docs_from_folders(root_dir, max_docs=MAX_DOCS_FOR_TESTING)\n",
        "    print(f\"[SUCCESS] Enhanced loading complete. Found {len(docs)} structured documents.\")\n",
        "    if docs:\n",
        "        print(\"\\n[DEBUG] Sample metadata from the first document:\")\n",
        "        # Pretty print the JSON metadata for readability\n",
        "        print(json.dumps(docs[0].metadata, indent=2, ensure_ascii=False))\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\\n[ERROR] Enhanced loading process failed: {e}\")\n",
        "    print(\"[INFO] Attempting fallback to simple directory loading...\")\n",
        "    \n",
        "    try:\n",
        "        # Fallback to simple loading with limit\n",
        "        loader = DirectoryLoader(\n",
        "            root_dir, \n",
        "            glob='**/*.txt', \n",
        "            loader_cls=TextLoader, \n",
        "            show_progress=True # Langchain's built-in progress bar\n",
        "        )\n",
        "        all_docs = loader.load()\n",
        "        docs = all_docs[:MAX_DOCS_FOR_TESTING]\n",
        "        print(f'[SUCCESS] Simple loading complete. Loaded {len(docs)} documents (capped at {MAX_DOCS_FOR_TESTING}).')\n",
        "    except Exception as fallback_e:\n",
        "        print(f\"[ERROR] Fallback loading also failed: {fallback_e}\")\n",
        "        docs = [] # Ensure docs is an empty list on total failure\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"   FINAL SUMMARY\")\n",
        "if docs:\n",
        "    print(f\"  -> Total documents loaded: {len(docs)}\")\n",
        "    print(\"  -> Status: Ready for next processing steps.\")\n",
        "    print(\"\\nPreview of first document content:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"{docs[0].page_content[:300]}...\")\n",
        "    print(\"-\" * 40)\n",
        "else:\n",
        "    print(\"  -> No documents were loaded. Please check the source directory and error logs.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "733d6611",
      "metadata": {},
      "source": [
        "# III. Document Splitting (Parent-Child Architecture)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "215f366d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Intelligent Chunking for Legal Documents\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1b892ba7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intelligent Legal Chunker initialized successfully!\n",
            "Features:\n",
            "- Respects 3500 token limit (safe for 4096 API limit)\n",
            "- Preserves paragraph boundaries\n",
            "- Handles oversized paragraphs intelligently\n",
            "- Maintains context with smart overlap\n"
          ]
        }
      ],
      "source": [
        "class IntelligentLegalChunker:\n",
        "    \"\"\"\n",
        "    Advanced chunking strategy for legal documents that:\n",
        "    1. Respects token limits (max 3500 tokens to stay under 4096 limit)\n",
        "    2. Preserves paragraph boundaries\n",
        "    3. Handles oversized paragraphs intelligently\n",
        "    4. Maintains context between chunks\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_tokens=3500, overlap_tokens=200):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.overlap_tokens = overlap_tokens\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def split_paragraph_intelligently(self, paragraph, max_tokens=None):\n",
        "        \"\"\"Split a single paragraph that's too large\"\"\"\n",
        "        if max_tokens is None:\n",
        "            max_tokens = self.max_tokens\n",
        "            \n",
        "        # Try to split on sentences first\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', paragraph)\n",
        "        if len(sentences) <= 1:\n",
        "            # If no sentence breaks, split on words\n",
        "            words = paragraph.split()\n",
        "            chunks = []\n",
        "            current_chunk = []\n",
        "            current_tokens = 0\n",
        "            \n",
        "            for word in words:\n",
        "                word_tokens = self.tokenizer.encode(word, disallowed_special=())\n",
        "                if current_tokens + len(word_tokens) > max_tokens and current_chunk:\n",
        "                    chunks.append(' '.join(current_chunk))\n",
        "                    current_chunk = [word]\n",
        "                    current_tokens = len(word_tokens)\n",
        "                else:\n",
        "                    current_chunk.append(word)\n",
        "                    current_tokens += len(word_tokens)\n",
        "            \n",
        "            if current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "            return chunks\n",
        "        \n",
        "        # Split by sentences\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_tokens = 0\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = self.tokenizer.encode(sentence, disallowed_special=())\n",
        "            if current_tokens + len(sentence_tokens) > max_tokens and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_tokens = len(sentence_tokens)\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_tokens += len(sentence_tokens)\n",
        "        \n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "        return chunks\n",
        "    \n",
        "    def chunk_document(self, document):\n",
        "        \"\"\"Chunk a document intelligently\"\"\"\n",
        "        content = document.page_content\n",
        "        metadata = document.metadata.copy()\n",
        "        \n",
        "        # Split by double newlines (paragraphs)\n",
        "        paragraphs = content.split('\\n\\n')\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_tokens = 0\n",
        "        \n",
        "        for paragraph in paragraphs:\n",
        "            paragraph = paragraph.strip()\n",
        "            if not paragraph:\n",
        "                continue\n",
        "                \n",
        "            paragraph_tokens = self.tokenizer.encode(paragraph, disallowed_special=())\n",
        "            \n",
        "            # If single paragraph is too large, split it\n",
        "            if len(paragraph_tokens) > self.max_tokens:\n",
        "                # Add current chunk if it exists\n",
        "                if current_chunk:\n",
        "                    chunks.append('\\n\\n'.join(current_chunk))\n",
        "                    current_chunk = []\n",
        "                    current_tokens = 0\n",
        "                \n",
        "                # Split the oversized paragraph\n",
        "                sub_chunks = self.split_paragraph_intelligently(paragraph)\n",
        "                for sub_chunk in sub_chunks:\n",
        "                    chunks.append(sub_chunk)\n",
        "                continue\n",
        "            \n",
        "            # Check if adding this paragraph would exceed limit\n",
        "            if current_tokens + len(paragraph_tokens) > self.max_tokens and current_chunk:\n",
        "                # Save current chunk\n",
        "                chunks.append('\\n\\n'.join(current_chunk))\n",
        "                \n",
        "                # Start new chunk with overlap if possible\n",
        "                if len(current_chunk) > 0:\n",
        "                    # Try to include last paragraph for overlap\n",
        "                    overlap_text = current_chunk[-1]\n",
        "                    overlap_tokens = self.tokenizer.encode(overlap_text, disallowed_special=())\n",
        "                    if len(overlap_tokens) <= self.overlap_tokens:\n",
        "                        current_chunk = [overlap_text, paragraph]\n",
        "                        current_tokens = len(overlap_tokens) + len(paragraph_tokens)\n",
        "                    else:\n",
        "                        current_chunk = [paragraph]\n",
        "                        current_tokens = len(paragraph_tokens)\n",
        "                else:\n",
        "                    current_chunk = [paragraph]\n",
        "                    current_tokens = len(paragraph_tokens)\n",
        "            else:\n",
        "                current_chunk.append(paragraph)\n",
        "                current_tokens += len(paragraph_tokens)\n",
        "        \n",
        "        # Add final chunk\n",
        "        if current_chunk:\n",
        "            chunks.append('\\n\\n'.join(current_chunk))\n",
        "        \n",
        "        # Create Document objects for each chunk\n",
        "        chunk_docs = []\n",
        "        for i, chunk_content in enumerate(chunks):\n",
        "            chunk_metadata = metadata.copy()\n",
        "            chunk_metadata['chunk_id'] = i\n",
        "            chunk_metadata['total_chunks'] = len(chunks)\n",
        "            chunk_docs.append(Document(page_content=chunk_content, metadata=chunk_metadata))\n",
        "        \n",
        "        return chunk_docs\n",
        "\n",
        "print(\"Intelligent Legal Chunker initialized successfully!\")\n",
        "print(\"Features:\")\n",
        "print(\"- Respects 3500 token limit (safe for 4096 API limit)\")\n",
        "print(\"- Preserves paragraph boundaries\")\n",
        "print(\"- Handles oversized paragraphs intelligently\")\n",
        "print(\"- Maintains context with smart overlap\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4f5ee215",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up enhanced parent-child document architecture with intelligent chunking...\n",
            "Processing 50 documents with intelligent chunking...\n",
            "Processing document 1/50: Nghị quyết 02/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 2/50: Nghị quyết 05/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 3/50: Nghị quyết 06/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 4/50: Nghị quyết 06/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 5/50: Nghị quyết 07/2024/NQ-HĐND quy định mức chi bảo đả...\n",
            "Processing document 6/50: Nghị quyết 07/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 7/50: Nghị quyết 10/2024/NQ-HĐND quy định một số chính s...\n",
            "Processing document 8/50: Nghị quyết 10/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 9/50: Nghị quyết 11/2024/NQ-HĐND quy định chính sách hỗ ...\n",
            "Processing document 10/50: Nghị quyết 11/2024/NQ-HĐND quy định mức chi đảm bả...\n",
            "Processing document 11/50: Nghị quyết 11/2024/NQ-HĐND về Quy định nội dung, m...\n",
            "Processing document 12/50: Nghị quyết 126/2024/NQ-HĐND quy định mức thu học p...\n",
            "Processing document 13/50: Nghị quyết 13/2024/NQ-HĐND quy định mức chi bảo đả...\n",
            "Processing document 14/50: Nghị quyết 15/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 15/50: Nghị quyết 15/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 16/50: Nghị quyết 15/2024/NQ-HĐND về Quy định chính sách ...\n",
            "Processing document 17/50: Nghị quyết 16/2024/NQ-HĐND quy định mức học phí đố...\n",
            "Processing document 18/50: Nghị quyết 16/2024/NQ-HĐND về Quy định nội dung ch...\n",
            "Processing document 19/50: Nghị quyết 17/2024/NQ-HĐND quy định các khoản thu ...\n",
            "Processing document 20/50: Nghị quyết 17/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 21/50: Nghị quyết 36/NQ-HĐND năm 2024 điều chỉnh Nghị quy...\n",
            "Processing document 22/50: Nghị quyết 39/2024/NQ-HĐND quy định mức chi bảo đả...\n",
            "Processing document 23/50: Nghị quyết 49/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 24/50: Nghị quyết 50/2024/NQ-HĐND hỗ trợ học phí đối với ...\n",
            "Processing document 25/50: Nghị quyết 58/2024/NQ-HĐND quy định mức thu học ph...\n",
            "Processing document 26/50: Nghị quyết 74/2024/NQ-HĐND quy định về học phí từ ...\n",
            "Processing document 27/50: Nghị quyết 82/2024/NQ-HĐND quy định nội dung chi, ...\n",
            "Processing document 28/50: Nghị quyết 83/2024/NQ-HĐND quy định nội dung, mức ...\n",
            "Processing document 29/50: Quyết định 1287/QĐ-UBND năm 2024 về Kế hoạch thời ...\n",
            "Processing document 30/50: Quyết định 1299/QĐ-UBND năm 2024 về Kế hoạch thời ...\n",
            "Processing document 31/50: Quyết định 1370/QĐ-UBND năm 2024 về Kế hoạch thời ...\n",
            "Processing document 32/50: Quyết định 1419/QĐ-UBND năm 2024 về Kế hoạch thời ...\n",
            "Processing document 33/50: Quyết định 1427/QĐ-UBND năm 2024 phê duyệt bổ sung...\n",
            "Processing document 34/50: Quyết định 1627/QĐ-UBND năm 2024 về Khung kế hoạch...\n",
            "Processing document 35/50: Quyết định 1828/QĐ-UBND năm 2024 về Kế hoạch thời ...\n",
            "Processing document 36/50: Quyết định 1894/QĐ-UBND năm 2024 phê duyệt kế hoạc...\n",
            "Processing document 37/50: Quyết định 1957/QĐ-BGDĐT xác định ngưỡng đảm bảo c...\n",
            "Processing document 38/50: Quyết định 1958/QĐ-BGDĐT xác định ngưỡng đảm bảo c...\n",
            "Processing document 39/50: Quyết định 1962/QĐ-BVHTTDL năm 2024 về Kế hoạch tr...\n",
            "Processing document 40/50: Quyết định 2012/QĐ-BGDĐT năm 2024 về Kế hoạch rà s...\n",
            "Processing document 41/50: Quyết định 2045/QĐ-BGDĐT năm 2024 về Khung kế hoạc...\n",
            "Processing document 42/50: Quyết định 2060/QĐ-BGDĐT năm 2024 công bố danh mục...\n",
            "Processing document 43/50: Quyết định 2826/QĐ-UBND năm 2024 về Khung kế hoạch...\n",
            "Processing document 44/50: Quyết định 2834/QĐ-UBND năm 2024 về Kế hoạch thời ...\n",
            "Processing document 45/50: Quyết định 3048/QĐ-UBND năm 2024 công bố thủ tục h...\n",
            "Processing document 46/50: Quyết định 3089/QĐ-UBND năm 2024 về Kế hoạch thời ...\n",
            "Processing document 47/50: Quyết định 50/2024/QĐ-UBND về Định mức kinh tế - k...\n",
            "Processing document 48/50: Quyết định 654/QĐ-UBND năm 2024 về Kế hoạch thời g...\n",
            "Processing document 49/50: Quyết định 944/QĐ-UBND năm 2024 về Kế hoạch thời g...\n",
            "Processing document 50/50: Quyết định 994/QĐ-UBND năm 2024 về Khung kế hoạch ...\n",
            "[INFO] Created 546 child chunks from 50 documents.\n",
            "[INFO] Parent chunks stored in docstore for context retrieval.\n",
            "[INFO] Chunk size statistics:\n",
            "   - Min tokens: 17\n",
            "   - Max tokens: 2770\n",
            "   - Avg tokens: 661\n",
            "   - Chunks over 3500 tokens: 0\n",
            "[SUCCESS] All chunks are within safe token limits.\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Parent-Child Architecture with Intelligent Chunking\n",
        "print(\"Setting up enhanced parent-child document architecture with intelligent chunking...\")\n",
        "\n",
        "# Initialize the intelligent chunker\n",
        "intelligent_chunker = IntelligentLegalChunker(max_tokens=3500, overlap_tokens=200)\n",
        "\n",
        "# Create parent chunks and store them\n",
        "docstore = InMemoryStore()\n",
        "child_chunks = []\n",
        "\n",
        "print(f\"Processing {len(docs)} documents with intelligent chunking...\")\n",
        "\n",
        "for doc_idx, doc in enumerate(docs):\n",
        "    print(f\"Processing document {doc_idx + 1}/{len(docs)}: {doc.metadata.get('title', 'Unknown')[:50]}...\")\n",
        "    \n",
        "    # Use intelligent chunking for parent chunks\n",
        "    parent_chunks = intelligent_chunker.chunk_document(doc)\n",
        "    \n",
        "    for i, parent_chunk in enumerate(parent_chunks):\n",
        "        parent_id = f\"{doc.metadata.get('title', 'doc')}-{i}\"\n",
        "        docstore.mset([(parent_id, parent_chunk)])\n",
        "        \n",
        "        # For child chunks, use smaller intelligent chunking\n",
        "        child_chunker = IntelligentLegalChunker(max_tokens=800, overlap_tokens=100)\n",
        "        sub_chunks = child_chunker.chunk_document(parent_chunk)\n",
        "        \n",
        "        for sub_chunk in sub_chunks:\n",
        "            sub_chunk.metadata['parent_id'] = parent_id\n",
        "            # Preserve original metadata\n",
        "            sub_chunk.metadata.update(doc.metadata)\n",
        "            child_chunks.append(sub_chunk)\n",
        "\n",
        "print(f\"[INFO] Created {len(child_chunks)} child chunks from {len(docs)} documents.\")\n",
        "print(f\"[INFO] Parent chunks stored in docstore for context retrieval.\")\n",
        "\n",
        "# Verify chunk sizes\n",
        "chunk_sizes = [tiktoken_len(chunk.page_content) for chunk in child_chunks]\n",
        "print(f\"[INFO] Chunk size statistics:\")\n",
        "print(f\"   - Min tokens: {min(chunk_sizes)}\")\n",
        "print(f\"   - Max tokens: {max(chunk_sizes)}\")\n",
        "print(f\"   - Avg tokens: {sum(chunk_sizes)//len(chunk_sizes)}\")\n",
        "print(f\"   - Chunks over 3500 tokens: {sum(1 for size in chunk_sizes if size > 3500)}\")\n",
        "\n",
        "if max(chunk_sizes) > 3500:\n",
        "    print(f\"[WARNING] Some chunks still exceed 3500 tokens.\")\n",
        "    print(\"   This might cause API errors. Consider reducing max_tokens further.\")\n",
        "else:\n",
        "    print(\"[SUCCESS] All chunks are within safe token limits.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4111ca6",
      "metadata": {},
      "source": [
        "# IV. Embedding Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "79002d2a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced ProtonXEmbeddings class updated with robust dynamic batching and internal progress bar.\n"
          ]
        }
      ],
      "source": [
        "# Enhanced ProtonXEmbeddings with Token Limit Handling\n",
        "class EnhancedProtonXEmbeddings(Embeddings):\n",
        "    \"\"\"\n",
        "    Enhanced ProtonX embeddings class that handles token limit errors gracefully.\n",
        "    Features:\n",
        "    - Automatic text truncation for oversized inputs\n",
        "    - Dynamic, token-aware batching to avoid API token limits\n",
        "    - Internal progress bar for user feedback\n",
        "    \"\"\"\n",
        "    def __init__(self, max_retries: int = 3, delay: int = 5, batch_token_limit: int = 3500):\n",
        "        \"\"\"Initializes the ProtonX client and sets processing parameters.\"\"\"\n",
        "        self.client = ProtonX()\n",
        "        self.max_retries = max_retries\n",
        "        self.delay = delay\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        self.batch_token_limit = batch_token_limit\n",
        "        print(f\"Enhanced ProtonXEmbeddings initialized with a token limit of {self.batch_token_limit} per batch.\")\n",
        "\n",
        "    def _get_token_count(self, text: str) -> int:\n",
        "        \"\"\"Calculates the number of tokens in a string.\"\"\"\n",
        "        return len(self.tokenizer.encode(text, disallowed_special=()))\n",
        "\n",
        "    def _truncate_text_if_needed(self, text: str, max_tokens: int) -> str:\n",
        "        \"\"\"Truncate text if it exceeds token limit\"\"\"\n",
        "        if self._get_token_count(text) <= max_tokens:\n",
        "            return text\n",
        "\n",
        "        tokens = self.tokenizer.encode(text, disallowed_special=())\n",
        "        truncated_tokens = tokens[:max_tokens]\n",
        "        print(f\"[WARNING] A single document was truncated as it exceeds the batch token limit.\")\n",
        "        return self.tokenizer.decode(truncated_tokens)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Generates embeddings for a list of documents using a built-in progress bar and dynamic batching.\n",
        "        \"\"\"\n",
        "        all_embeddings = []\n",
        "        current_batch = []\n",
        "        current_batch_tokens = 0\n",
        "\n",
        "        # Wrap the texts iterator with tqdm for a progress bar\n",
        "        for text in tqdm(texts, desc=\"Generating Embeddings\", unit=\"chunk\"):\n",
        "            truncated_text = self._truncate_text_if_needed(text, self.batch_token_limit)\n",
        "            text_tokens = self._get_token_count(truncated_text)\n",
        "\n",
        "            if current_batch and (current_batch_tokens + text_tokens > self.batch_token_limit):\n",
        "                batch_embeddings = self._process_batch_with_retries(current_batch)\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "                current_batch = [truncated_text]\n",
        "                current_batch_tokens = text_tokens\n",
        "            else:\n",
        "                current_batch.append(truncated_text)\n",
        "                current_batch_tokens += text_tokens\n",
        "\n",
        "        if current_batch:\n",
        "            batch_embeddings = self._process_batch_with_retries(current_batch)\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "        return all_embeddings\n",
        "\n",
        "    def _process_batch_with_retries(self, batch: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Handles the API call with retries for a given batch.\"\"\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                response = self.client.embeddings.create(input=batch)\n",
        "                if not response or not response.get(\"data\"):\n",
        "                    raise ValueError(\"Invalid response from ProtonX API\")\n",
        "                return [item.get(\"embedding\") for item in response.get(\"data\")]\n",
        "            except Exception as e:\n",
        "                print(f\"\\n[ERROR] API call failed on attempt {attempt + 1}/{self.max_retries}: {e}\")\n",
        "                if attempt + 1 == self.max_retries:\n",
        "                    print(f\"[ERROR] All retries failed for batch. Returning zero vectors.\")\n",
        "                    return [[0.0] * 1536 for _ in batch]\n",
        "                time.sleep(self.delay)\n",
        "        return []\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Generates an embedding for a single query text.\"\"\"\n",
        "        truncated_text = self._truncate_text_if_needed(text, self.batch_token_limit)\n",
        "        return self._process_batch_with_retries([truncated_text])[0]\n",
        "\n",
        "print(\"Enhanced ProtonXEmbeddings class updated with robust dynamic batching and internal progress bar.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1ce71baf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced ProtonXEmbeddings initialized with a token limit of 3500 per batch.\n",
            "Enhanced ProtonX embeddings object created and ready to use.\n",
            "\n",
            "======================================================================\n",
            "   STARTING EMBEDDING GENERATION PROCESS\n",
            "======================================================================\n",
            "[INFO] Number of text chunks to process: 546\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Embeddings: 100%|██████████| 546/546 [19:00<00:00,  2.09s/chunk]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "   EMBEDDING PROCESS COMPLETE: SUMMARY REPORT\n",
            "======================================================================\n",
            "[SUCCESS] Process finished in 1146.44 seconds (19.1 minutes).\n",
            "\n",
            "--- Performance Metrics ---\n",
            "  -> Total chunks processed   : 546\n",
            "  -> Throughput               : 0.5 chunks/sec\n",
            "  -> Avg. time per chunk      : 2099.71 ms\n",
            "\n",
            "--- Embedding Details ---\n",
            "  -> Embedding dimension      : 768\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# 1. Initialize the enhanced embeddings class\n",
        "embeddings = EnhancedProtonXEmbeddings()\n",
        "print(\"Enhanced ProtonX embeddings object created and ready to use.\")\n",
        "\n",
        "# 2. Extract texts and metadatas from child chunks\n",
        "texts = [chunk.page_content for chunk in child_chunks]\n",
        "metadatas = [chunk.metadata for chunk in child_chunks]\n",
        "\n",
        "# --- START OF PROCESS ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"   STARTING EMBEDDING GENERATION PROCESS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"[INFO] Number of text chunks to process: {len(texts)}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 3. Process all embeddings with a single call.\n",
        "# The new class handles its own batching and progress bar internally.\n",
        "text_embeddings = embeddings.embed_documents(texts)\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# --- FINAL SUMMARY REPORT ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"   EMBEDDING PROCESS COMPLETE: SUMMARY REPORT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if text_embeddings and len(text_embeddings) == len(texts):\n",
        "    print(f\"[SUCCESS] Process finished in {total_time:.2f} seconds ({total_time/60:.1f} minutes).\")\n",
        "\n",
        "    print(\"\\n--- Performance Metrics ---\")\n",
        "    print(f\"  -> {'Total chunks processed':<25}: {len(text_embeddings)}\")\n",
        "    print(f\"  -> {'Throughput':<25}: {len(text_embeddings)/total_time:.1f} chunks/sec\")\n",
        "    print(f\"  -> {'Avg. time per chunk':<25}: {(total_time/len(text_embeddings))*1000:.2f} ms\")\n",
        "\n",
        "    print(\"\\n--- Embedding Details ---\")\n",
        "    print(f\"  -> {'Embedding dimension':<25}: {len(text_embeddings[0])}\")\n",
        "else:\n",
        "    print(\"[ERROR] Embedding generation failed or was incomplete. Please check the error logs above.\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d96e8597",
      "metadata": {},
      "source": [
        "# V. Vector Store and Retriever Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c91622f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating FAISS vector store...\n",
            "FAISS vector store created successfully.\n",
            "\n",
            "Setting up BM25 retriever with Vietnamese tokenization...\n",
            "BM25 retriever with Vietnamese tokenization created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Setup FAISS vector store\n",
        "print(\"\\nCreating FAISS vector store...\")\n",
        "# FAISS requires (text, embedding) pairs\n",
        "text_embedding_pairs = list(zip(texts, text_embeddings))\n",
        "vectorstore_local = FAISS.from_embeddings(text_embedding_pairs, embeddings, metadatas=metadatas)\n",
        "print(\"FAISS vector store created successfully.\")\n",
        "\n",
        "# Setup BM25 retriever with Vietnamese tokenization\n",
        "print(\"\\nSetting up BM25 retriever with Vietnamese tokenization...\")\n",
        "\n",
        "def tokenize_vi_for_bm25(doc):\n",
        "    \"\"\"Tokenize Vietnamese text for better BM25 matching\"\"\"\n",
        "    doc.page_content = ViTokenizer.tokenize(doc.page_content)\n",
        "    return doc\n",
        "\n",
        "# Create tokenized copies for BM25\n",
        "child_chunks_tokenized = [tokenize_vi_for_bm25(chunk) for chunk in child_chunks]\n",
        "bm25_retriever = BM25Retriever.from_documents(child_chunks_tokenized)\n",
        "bm25_retriever.k = 15  # Retrieve top 15 for ensemble\n",
        "print(\"BM25 retriever with Vietnamese tokenization created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b1c0350",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Setting up ensemble retrieval...\n",
            "Ensemble retriever (BM25 + FAISS) created successfully.\n",
            "Parent document retrieval chain created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Setup Ensemble Retrieval (BM25 + FAISS)\n",
        "print(\"\\nSetting up ensemble retrieval...\")\n",
        "\n",
        "# Create FAISS retriever\n",
        "faiss_retriever = vectorstore_local.as_retriever(search_kwargs={'k': 15})\n",
        "\n",
        "# Create ensemble retriever combining BM25 and FAISS\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever],\n",
        "    weights=[0.6, 0.4]  # Favor BM25 for Vietnamese lexical matching\n",
        ")\n",
        "\n",
        "print(\"Ensemble retriever (BM25 + FAISS) created successfully.\")\n",
        "\n",
        "# Setup Parent Document Retrieval Chain\n",
        "def _get_parent_docs(input_dict: dict) -> list[Document]:\n",
        "    \"\"\"Retrieve parent documents from child documents\"\"\"\n",
        "    child_docs = input_dict[\"child_docs\"]\n",
        "    store = input_dict[\"docstore\"]\n",
        "    parent_ids = []\n",
        "    for doc in child_docs:\n",
        "        if \"parent_id\" in doc.metadata and doc.metadata[\"parent_id\"] not in parent_ids:\n",
        "            parent_ids.append(doc.metadata[\"parent_id\"])\n",
        "    return [doc for doc in store.mget(parent_ids) if doc is not None]\n",
        "\n",
        "# Create the parent retrieval chain\n",
        "parent_retriever_chain = (\n",
        "    {\"child_docs\": ensemble_retriever}\n",
        "    | RunnablePassthrough.assign(docstore=lambda x: docstore)\n",
        "    | RunnableLambda(_get_parent_docs)\n",
        ")\n",
        "\n",
        "print(\"Parent document retrieval chain created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b798d92",
      "metadata": {},
      "source": [
        "# VI. RAG Chain Setup and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f95a24c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced RAG chain with hybrid retrieval and parent documents created successfully.\n"
          ]
        }
      ],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0)\n",
        "\n",
        "# Enhanced prompt template with Vietnamese instructions and citation requirements\n",
        "template = \"\"\"Answer the question based ONLY on the following context.\n",
        "Your answer must be in Vietnamese.\n",
        "Your answer should be well-structured and easy to read.\n",
        "- Use bullet points or numbered lists for multiple items or steps.\n",
        "- Use **bold** for key terms, names, or important numbers and concepts.\n",
        "- Use *italics* for emphasis or to highlight specific terms.\n",
        "\n",
        "**After providing the answer, you MUST cite your sources accurately using the metadata from the context.**\n",
        "For each source used, provide its title and document number (Số hiệu) if available.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "If the context does not provide enough information, say \"Tôi không tìm thấy thông tin trong tài liệu được cung cấp.\" and do not provide an answer.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    \"\"\"Format retrieved documents with metadata for better context\"\"\"\n",
        "    formatted_docs = []\n",
        "    for i, doc in enumerate(docs):\n",
        "        # Add document metadata as header\n",
        "        title = doc.metadata.get('title', f'Document {i+1}')\n",
        "        source = doc.metadata.get('source', 'Unknown source')\n",
        "        formatted_doc = f\"--- Document: {title} ---\\n{doc.page_content}\\n--- End Document ---\"\n",
        "        formatted_docs.append(formatted_doc)\n",
        "    return \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "# Enhanced RAG chain using parent document retrieval\n",
        "rag_chain = (\n",
        "    {\"context\": parent_retriever_chain | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Enhanced RAG chain with hybrid retrieval and parent documents created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7a6dcc49",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: thời gian học tập năm 2024-2025 tỉnh bắc kạn là từ khi nào?\n",
            "\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Response:** Dựa trên các tài liệu được cung cấp, kế hoạch thời gian học tập năm học 2024-2025 đối với giáo dục mầm non, giáo dục phổ thông và giáo dục thường xuyên tại tỉnh Bắc Kạn được quy định như sau:\n",
              "\n",
              "*   **Ngày tựu trường:**\n",
              "    *   Các cấp học tựu trường sớm nhất vào **ngày 29 tháng 8 năm 2024** (thứ Năm).\n",
              "    *   Riêng đối với lớp 1, tựu trường sớm nhất vào **ngày 22 tháng 8 năm 2024** (thứ Năm).\n",
              "\n",
              "*   **Ngày khai giảng:**\n",
              "    *   Tổ chức khai giảng năm học mới vào **ngày 05 tháng 9 năm 2024** (thứ Năm).\n",
              "\n",
              "*   **Kế hoạch học kỳ và kết thúc năm học:**\n",
              "    *   Năm học có **35 tuần thực học**, trong đó học kỳ I có 18 tuần và học kỳ II có 17 tuần.\n",
              "    *   Kết thúc học kỳ I trước **ngày 18 tháng 01 năm 2025**.\n",
              "    *   Hoàn thành chương trình và kết thúc năm học trước **ngày 31 tháng 5 năm 2025**.\n",
              "\n",
              "*   **Các mốc thời gian quan trọng khác:**\n",
              "    *   Xét công nhận hoàn thành chương trình tiểu học và tốt nghiệp trung học cơ sở trước **ngày 30 tháng 6 năm 2025**.\n",
              "    *   Hoàn thành tuyển sinh các lớp đầu cấp trước **ngày 31 tháng 7 năm 2025**.\n",
              "    *   Thi tốt nghiệp trung học phổ thông năm 2025 dự kiến diễn ra vào **ngày 26 và 27 tháng 6 năm 2025**.\n",
              "\n",
              "***\n",
              "\n",
              "**Nguồn:**\n",
              "*   Quyết định 1370/QĐ-UBND năm 2024 về Kế hoạch thời gian năm học 2024-2025 đối với giáo dục mầm non, giáo dục phổ thông và giáo dục thường xuyên tỉnh Bắc Kạn (Số hiệu: 1370/QĐ-UBND)."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"thời gian học tập năm 2024-2025 tỉnh bắc kạn là từ khi nào?\"\n",
        "print(f\"Query: {query}\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "response = rag_chain.invoke(query)\n",
        "# print(f\"Response: {response}\\n\")\n",
        "display(Markdown(f\"**Response:** {response}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "36ed319c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Kỳ thi chọn học sinh giỏi cấp tỉnh lớp 12 được tổ chức vào ngày nào?\n",
            "\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Response:** Dựa trên các tài liệu được cung cấp, thời gian tổ chức kỳ thi chọn học sinh giỏi cấp tỉnh lớp 12 năm học 2024-2025 khác nhau tùy theo từng địa phương. Cụ thể như sau:\n",
              "\n",
              "*   **Tỉnh Đắk Nông**: Kỳ thi học sinh giỏi lớp 12 trung học phổ thông cấp tỉnh được tổ chức từ ngày **25 tháng 3 năm 2025** đến ngày **26 tháng 3 năm 2025**.\n",
              "*   **Tỉnh Bình Định**: Kỳ thi chọn học sinh giỏi cấp tỉnh lớp 12 được tổ chức vào ngày **22/10/2024**.\n",
              "*   **Tỉnh Lâm Đồng**: Hoàn thành tổ chức thi chọn học sinh giỏi cấp tỉnh lớp 12 trước ngày **31/01/2025**.\n",
              "*   **Tỉnh Yên Bái**: Thi chọn học sinh giỏi trung học phổ thông cấp tỉnh phải hoàn thành trước ngày **31 tháng 3 năm 2025**.\n",
              "*   **Tỉnh Bắc Kạn**: Thi chọn học sinh giỏi văn hóa cấp tỉnh trung học phổ thông phải hoàn thành trước ngày **28 tháng 4 năm 2025**.\n",
              "*   **Tỉnh Sóc Trăng**: Thời gian thi sẽ theo Kế hoạch của Sở Giáo dục và Đào tạo.\n",
              "*   **Tỉnh Quảng Ngãi**: Thời gian thi sẽ do Sở Giáo dục và Đào tạo hướng dẫn cụ thể.\n",
              "\n",
              "***\n",
              "\n",
              "### Nguồn tài liệu:\n",
              "1.  Quyết định 944/QĐ-UBND năm 2024 về Kế hoạch thời gian năm học 2024-2025 đối với giáo dục mầm non, giáo dục phổ thông và giáo dục thường xuyên do tỉnh Đắk Nông ban hành (Số hiệu: 944/QĐ-UBND).\n",
              "2.  Quyết định 2834/QĐ-UBND năm 2024 về Kế hoạch thời gian năm học 2024-2025 đối với giáo dục mầm non, giáo dục phổ thông và giáo dục thường xuyên trên địa bàn tỉnh Bình Định (Số hiệu: 2834/QĐ-UBND).\n",
              "3.  Quyết định 1287/QĐ-UBND năm 2024 về Kế hoạch thời gian năm học 2024-2025 đối với giáo dục mầm non, giáo dục phổ thông và giáo dục thường xuyên trên địa bàn tỉnh Lâm Đồng (Số hiệu: 1287/QĐ-UBND).\n",
              "4.  Quyết định 1627/QĐ-UBND năm 2024 về Khung kế hoạch thời gian năm học 2024-2025 đối với giáo dục mầm non, giáo dục phổ thông và giáo dục thường xuyên trên địa bàn tỉnh Yên Bái (Số hiệu: 1627/QĐ-UBND).\n",
              "5.  Quyết định 1370/QĐ-UBND năm 2024 về Kế hoạch thời gian năm học 2024-2025 đối với giáo dục mầm non, giáo dục phổ thông và giáo dục thường xuyên tỉnh Bắc Kạn (Số hiệu: 1370/QĐ-UBND).\n",
              "6.  Quyết định 1828/QĐ-UBND năm 2024 về Kế hoạch thời gian năm học 2024-2025 đối với giáo dục mầm non, giáo dục phổ thông và giáo dục thường xuyên trên địa bàn tỉnh Sóc Trăng (Số hiệu: 1828/QĐ-UBND).\n",
              "7.  Quyết định 994/QĐ-UBND năm 2024 về Khung kế hoạch thời gian năm học 2024-2025 đối với giáo dục mầm non, giáo dục phổ thông và giáo dục thường xuyên tỉnh Quảng Ngãi (Số hiệu: 994/QĐ-UBND)."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Kỳ thi chọn học sinh giỏi cấp tỉnh lớp 12 được tổ chức vào ngày nào?\"\n",
        "print(f\"Query: {query}\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "response = rag_chain.invoke(query)\n",
        "# print(f\"Response: {response}\\n\")\n",
        "display(Markdown(f\"**Response:** {response}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8adbd46d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Dựa trên hồ sơ do Công ty TNHH OTES CORPORATION cung cấp, hãy mô tả các đặc tính kỹ thuật chính của sản phẩm TamSoil Polynite ECO ở trạng thái dung dịch sau khi pha, và nêu rõ căn cứ pháp lý cao nhất (Luật) mà Cục Hải quan đã dựa vào để ban hành thông báo mã số cho sản phẩm này.\n",
            "\n",
            "================================================================================\n",
            "Response: Tôi không tìm thấy thông tin trong tài liệu được cung cấp.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Response:** Tôi không tìm thấy thông tin trong tài liệu được cung cấp."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Dựa trên hồ sơ do Công ty TNHH OTES CORPORATION cung cấp, hãy mô tả các đặc tính kỹ thuật chính của sản phẩm TamSoil Polynite ECO ở trạng thái dung dịch sau khi pha, và nêu rõ căn cứ pháp lý cao nhất (Luật) mà Cục Hải quan đã dựa vào để ban hành thông báo mã số cho sản phẩm này.\"\n",
        "print(f\"Query: {query}\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "response = rag_chain.invoke(query)\n",
        "print(f\"Response: {response}\\n\")\n",
        "\n",
        "# Display with markdown formatting\n",
        "display(Markdown(f\"**Response:** {response}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c54a9efb",
      "metadata": {},
      "source": [
        "# VII. Performance Comparison and Analysis\n",
        "\n",
        "## Enhanced Features Summary\n",
        "\n",
        "### ✅ **What's New in This Enhanced Version:**\n",
        "\n",
        "1. **🔍 Hybrid Retrieval**: BM25 (lexical) + FAISS (semantic) ensemble\n",
        "2. **🇻🇳 Vietnamese Tokenization**: Better BM25 matching for Vietnamese text\n",
        "3. **📚 Parent-Child Architecture**: Better context preservation\n",
        "4. **📄 Enhanced Data Loading**: Preserves structured metadata\n",
        "5. **🎯 Improved Prompting**: Vietnamese instructions with citation requirements\n",
        "6. **⚡ ProtonX Embeddings**: High-quality Vietnamese embeddings (free)\n",
        "\n",
        "### **Architecture Flow:**\n",
        "```\n",
        "Query → [BM25 + FAISS] → Ensemble → Parent-Child Hydration → Enhanced RAG → Response\n",
        "```\n",
        "\n",
        "### **Key Improvements Over Original:**\n",
        "- **Better Recall**: Hybrid retrieval catches both exact matches and semantic similarity\n",
        "- **Better Context**: Parent-child architecture provides more complete context\n",
        "- **Vietnamese-Optimized**: Tokenization and specialized embeddings\n",
        "- **Production-Ready**: Robust error handling and metadata preservation\n",
        "- **Citation Support**: Structured metadata for accurate source attribution\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fyp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

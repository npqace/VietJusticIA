
import os
import subprocess
import argparse
import sys
import shutil

# --- Configuration ---
# Define paths relative to the project root
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
AI_ENGINE_ROOT = os.path.join(PROJECT_ROOT)

# Scripts location
AI_SCRIPTS_DIR = os.path.join(AI_ENGINE_ROOT, 'scripts')

# Paths to the scripts in the pipeline
CRAWLER_SCRIPT = os.path.join(AI_ENGINE_ROOT, 'data', 'crawler', 'crawler.py')
CLEANER_SCRIPT = os.path.join(AI_ENGINE_ROOT, 'data', 'processing', 'run_cleaner.py')
MIGRATOR_SCRIPT = os.path.join(AI_SCRIPTS_DIR, 'migrate_to_mongo.py')
VECTOR_STORE_BUILDER_SCRIPT = os.path.join(AI_SCRIPTS_DIR, 'build_vector_store.py')

# Path to the data generated by the crawler
CRAWLER_OUTPUT_DIR = os.path.join(AI_ENGINE_ROOT, 'data', 'raw_data', 'documents')

def run_command(command, description):
    """Runs a command as a subprocess and logs its execution."""
    print(f"\n{'='*20}\n[PIPELINE] Running: {description}\n{'='*20}")
    try:
        process = subprocess.run(command, shell=True, check=True, text=True)
        print(f"[PIPELINE] SUCCESS: {description} completed.")
    except subprocess.CalledProcessError as e:
        print(f"[PIPELINE] ERROR: {description} failed with exit code {e.returncode}.")
        print(f"--> STDOUT: {e.stdout}")
        print(f"--> STDERR: {e.stderr}")
        sys.exit(1) # Exit the pipeline if a step fails

# def organize_scripts():
#     """Moves scripts to their correct locations if they are not already there."""
#     print(f"\n{'='*20}\n[PIPELINE] Organizing script locations...\n{'='*20}")
#     os.makedirs(AI_SCRIPTS_DIR, exist_ok=True)

#     scripts_to_move = {
#         'migrate_to_mongo.py': MIGRATOR_SCRIPT,
#         'build_vector_store.py': VECTOR_STORE_BUILDER_SCRIPT
#     }

#     for script_name, dest_path in scripts_to_move.items():
#         src_path = os.path.join(BACKEND_SCRIPTS_DIR, script_name)
#         if os.path.exists(src_path):
#             print(f"--> Moving {script_name} from {src_path} to {dest_path}")
#             shutil.move(src_path, dest_path)
#             print(f"--> Move successful.")
#         elif os.path.exists(dest_path):
#             print(f"--> {script_name} is already in the correct location.")
#         else:
#             print(f"--> WARNING: {script_name} not found at source or destination. Skipping.")
#     print("[PIPELINE] Script organization complete.")

def main():
    """Parses arguments and runs the full data pipeline."""
    parser = argparse.ArgumentParser(
        description="Master pipeline script to crawl, clean, and build the AI data stores.",
        formatter_class=argparse.RawTextHelpFormatter)
    
    # Crawler-specific arguments
    parser.add_argument('--max-docs', type=int, default=None, help='(Crawler) Max number of documents to scrape.')
    parser.add_argument('--max-pages', type=int, default=None, help='(Crawler) Max number of API pages to fetch.')
    parser.add_argument('--status-filter', type=str, default='Còn hiệu lực', help='(Crawler) Filter documents by status.')
    parser.add_argument('--category', type=str, default=None, help='(Crawler) Specify a category to scrape.')

    # Pipeline control arguments
    parser.add_argument('--force-rerun', action='store_true', help='Force all steps to re-run from scratch, ignoring existing data.')

    args = parser.parse_args()

    # 1. Organize scripts first
    # organize_scripts()

    # 2. Run Crawler
    # The crawler always fetches new data, so it doesn't have a --force flag.
    crawler_command = f"python {CRAWLER_SCRIPT}"
    if args.max_docs:
        crawler_command += f" --max-docs {args.max_docs}"
    if args.max_pages:
        crawler_command += f" --max-pages {args.max_pages}"
    if args.status_filter:
        crawler_command += f" --status-filter \"{args.status_filter}\""
    if args.category:
        crawler_command += f" --category \"{args.category}\""
    run_command(crawler_command, "Step 1: Crawling legal documents")

    # 3. Run Cleaner
    cleaner_command = f"python {CLEANER_SCRIPT} {CRAWLER_OUTPUT_DIR}"
    if args.force_rerun:
        cleaner_command += " --force"
    run_command(cleaner_command, "Step 2: Cleaning crawled text data")

    # 4. Run MongoDB Migrator
    migrator_command = f"python {MIGRATOR_SCRIPT}"
    if args.force_rerun:
        migrator_command += " --force"
    run_command(migrator_command, "Step 3: Migrating documents to MongoDB (with AI diagram generation)")

    # 5. Run Vector Store Builder
    vector_builder_command = f"python {VECTOR_STORE_BUILDER_SCRIPT}"
    if args.force_rerun:
        vector_builder_command += " --force-rebuild"
    run_command(vector_builder_command, "Step 4: Building Qdrant vector store")

    print(f"\n{'='*20}\n[PIPELINE] FULL DATA PIPELINE COMPLETED SUCCESSFULLY!\n{'='*20}")

if __name__ == "__main__":
    main()
